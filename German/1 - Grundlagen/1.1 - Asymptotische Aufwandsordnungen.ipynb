{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymptotische Aufwandsordnungen\n",
    "\n",
    "## Groß-O\n",
    "\n",
    "Die Groß-O ($\\mathcal{O}$) Notation ist eine mathematische Notation, die das Verhalten der Funktionswerte einer Funktion für größer werdende Argumente beschreibt. $\\mathcal{O}$ gehört zur Gruppe der Notationen, welche durch Paul Bachmann und Edmund Landau eingeführt wurden, und deshalb auch Bachmann-Landau Notationen genannt werden.\n",
    "\n",
    "In der Informatik wird die Groß-O Notation genutzt um Algorithmen bezüglich ihrer Laufzeit und ihres Speicherbedarfs zu klassifizieren.\n",
    "\n",
    "<div style=\"background: #f4f4f4; margin: 20px 40px; line-height: 2.2em; padding: 5px 15px;\">\n",
    "$f(n) = \\mathcal{O}(g(n))$ mit $n \\rightarrow \\infty$, wenn und nur dann, wenn \n",
    "<br>\n",
    "$\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 \\ldotp$ $f(n) \\leqslant c \\cdot g(n)$\n",
    "</div>\n",
    "\n",
    "Diese Definition sagt aus, dass $f$ genau dann zu $\\mathcal{O}(g)$ gehört, wenn es ein $c$ gibt, für das alle $f(n)$ ab einem bestimmten $n_0$ kleiner sind als $c \\cdot g(n)$. $c$ ist dabei eine reelle Zahl, welche größer als 0 ist. Die Funktion $g$, welche durch die Landau-Notation angegeben wird, dient also als obere Schranke für die Funktion $f$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/89/Big-O-notation.png\" alt=\"Drawing\" style=\"width:400px;\"/>\n",
    "\n",
    "Da es sich um eine obere Schranke handelt, gilt beispielsweise: $\\mathcal{O}(n) \\subset \\mathcal{O}(n^2)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(n^3)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(2^n)$. Es ist also auch korrekt anstatt $\\mathcal{O}(n)$ $\\mathcal{O}(n^2)$ anzugeben, dies wäre jedoch für die Praxis nicht sehr sinnvoll.\n",
    "\n",
    "Typische Laufzeiten sind: $\\mathcal{O}(1)$, $\\mathcal{O}(\\log n)$, $\\mathcal{O}(n)$, $\\mathcal{O}(n \\log n)$, $\\mathcal{O}(n^2)$, $\\mathcal{O}(n^3)$, $\\mathcal{O}(2^n)$ und $\\mathcal{O}(n!)$. Es gibt jedoch unendlich weitere mögliche Komplexitätsklassen. Außerdem muss es sich nicht immer um die Variable $n$ handeln, es können auch mehrere Variablen innerhalb einer Komplexitätslkasse vorkommen. Zum Beispiel ist der Aufwand, um eine Wand der Höhe $h$ und der Breite $b$ zu bemalen $\\mathcal{O}(hb)$.\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*yekzNjsqZzGCET2KotEROQ.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "## Groß-Omega\n",
    "\n",
    "Die Groß-Omega ($\\Omega$) Notation ist ein äquivalentes Konzept zu Groß-O, beschreibt jedoch eine untere Schranke. Die formale Defintion lautet demnach folgendermaßen:\n",
    "\n",
    "<div style=\"background: #f4f4f4; margin: 20px 40px; line-height: 2.2em; padding: 5px 15px;\">\n",
    "$f(n) = \\Omega(g(n))$ mit $n \\rightarrow \\infty$, wenn und nur dann, wenn \n",
    "<br>\n",
    "$\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : $ $f(n) \\geqslant c \\cdot g(n)$\n",
    "</div>\n",
    "\n",
    "$f$ gehört genau dann zu $\\Omega(g)$, wenn es ein $c$ gibt, für das alle $f(n)$ ab einem bestimmten $n_0$ größer sind als $c \\cdot g(n)$.\n",
    "\n",
    "## Groß-Theta\n",
    "\n",
    "Im Idealfall kann man eine asymptotische Beschrankung nach oben und unten durch ein und dieselbe Funktion mit verschiedenen Faktoren angeben. Grafisch wirkt dies wie ein Band, in dem die Graphen sämtlicher Funktionen aus $\\Theta(g)$ verlaufen. $\\Theta$ beschreibt damit die exakte Komplexitätsklasse.\n",
    "\n",
    "<div style=\"background: #f4f4f4; margin: 20px 40px; line-height: 2.2em; padding: 5px 15px;\">\n",
    "$f(n) = \\Theta(g(n))$ mit $n \\rightarrow \\infty$, wenn und nur dann, wenn \n",
    "<br>\n",
    "$\\exists c_1, c_2 \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : $ $c_1 \\cdot f(n) \\leqslant f(n) \\leqslant c_2 \\cdot g(n)$\n",
    "</div>\n",
    "\n",
    "In der Praxis wird meistens die $\\mathcal{O}$-Notation verwendet. Oft ist damit auch die exakte Aufwandordnung gemeint und wird somit anstatt $\\Theta$ verwendet, auch wenn dies korrekt wäre.\n",
    "\n",
    "## Best Case, Worst Case, Average Case\n",
    "\n",
    "Darüber hinaus gibt es drei Möglichkeiten die Laufzeit eines Algorithmus zu beschreiben. \n",
    "\n",
    "### Best Case\n",
    "\n",
    "Mit der __Best Case__ Laufzeit wird die schnellstmögliche Laufzeit angegeben. Sie tritt ein, wenn das zu lösende Problem am günstigsten ist. Bei [Selection Sort](../2%20-%20Datenstrukturen/Arrays.ipynb#Selection-Sort) beispielsweise ist dies der Fall, wenn Array (bzw. die Liste) bereits sortiert ist. Hierfür müsste lediglich einmal durch das Array traversiert werden und die Best Case Laufzeit beträgt somit $\\mathcal{O}(n)$. Da der Best Case in der Praxis so gut wie nie auftritt, ist dessen Angabe nicht von großer Bedeutung.\n",
    "\n",
    "### Worst Case\n",
    "\n",
    "Die __Worst Case__ Laufzeit ist hingegen wesentlich bedeutender. Sie gibt an, wie groß die Laufzeit des Algorithmus maximal werden kann, auch wenn das zu lösende Problem noch so ungünstig ist. Bei [Selection Sort](../2%20-%20Datenstrukturen/Arrays.ipynb#Selection-Sort) tritt der Worst Case ein, wenn es sich um ein ruckwärts sortiertes Array (von _groß_ nach _klein_) handelt. In diesem Fall müsste man immer durch das komplette Array gehen und kommt auf einen Aufwand von $\\mathcal{O}(n^2)$.\n",
    "\n",
    "### Average Case\n",
    "\n",
    "Nicht immer ist die Worst Case Angabe hilfreich. Was ist, wenn der Worst Case zwar bezüglich der Laufzeit sehr ungünstig ist, jedoch nur sehr selten eintritt? Hier ist die Angabe der Laufzeit im __Average Case__ bzw. __Expected Case__ repräsentativer. Für die Untersuchung von Average Case Laufzeiten bedarf es beispielsweise statistischen Methoden mit Wahrscheinlichkeiten oder der Amortisierten Analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vernachlässigen der Konstanten\n",
    "\n",
    "Bei der Angabe der Komplexitätsklasse werden Konstanten vernachlässigt. Eine Laufzeit, die als $\\mathcal{O}(2n)$ angegeben wurde, ist tatsächlich $\\mathcal{O}(n)$. Zum einen wird dies getan, da nur die Komplexitätsklasse interessiert, also eine ungefähre Einordnung des Verhalten der Laufzeit (bzw. des Speicheraufwands) in Abhängigkeit von $n$. Zum anderen wäre eine Angabe als $\\mathcal{O}(2n)$ keines Wegs genauer. Betrachtet man beispielsweise folgende Beispiele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    \n",
    "for n in lst2:\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte nun auf die Idee kommen, für das erste Beispiel $\\mathcal{O}(n)$ und für das zweite Beispiel $\\mathcal{O}(2n)$ anzugeben, da beim ersten Beispiel es eine Schleife mit $n$ Iterationen gibt und im zweiten zwei Schleifen mit $n$ Iterationen. Tatsächlich werden aber in beiden Beispielen gleich viele Operationen durchgeführt und eine Unterscheidung der Laufzeit würde hier keinen Sinn ergeben und zu falschen Schlüssen führen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial vs. Exponentiell\n",
    "\n",
    "Die Komplexitätsklassen lassen sich zwei fundamental unterschiedlichen Klassen zuordnen, der Klasse der polynomialen und die Klasse der exponentiellen Komplexitätsklassen.\n",
    "\n",
    "Lässt sich der Aufwand als Polynom in $n$ ausdrücken, so handelt es sich um polynomialen Aufwand. Das Polynom hat folgende Form: \n",
    "\n",
    "$$T(n) = \\sum_{i=0}^k (a_r n^r) = a_r n^r + a_{r-1} n^{r-1} + \\dotsc + a_1 n + a_0$$\n",
    "\n",
    "mit $r \\in \\mathbb{N}$, $a_0, \\dotsc, a_r \\in R$, $a_r \\neq 0$\n",
    "\n",
    "Da Logarithmus- und Wurzel-Funktionen, bzw. Funktionen mit nicht-ganzzahligen Exponenten, durch Polynome nach oben beschränkt werden können, zählen diese Funktionen auch zu den Polynomialfunktionen.\n",
    "\n",
    "Lässt sich der Aufwand $T$ nicht als Polynom, sondern in der Gestalt $T(n) = c \\cdot z^n$, mit $c, z \\in R$, $c \\neq 0$ und $z > 1$, angeben, so spricht man vom exponentiellen Aufwand.\n",
    "\n",
    "Für die Praxis ist diese Einteilung entscheidend, da man Algorithmen mit exponentiellen Aufwand als nicht praktikabel einstufen kann. Bei Algorithmen mit exponentiellen Zeitaufwand steigt die benötigte Zeit mit größer werdenden $n$ so  schnell, dass man einen so großen Zeitraum auf das Ergebnis warten müsste, dass der Algorithmus nutzlos ist.\n",
    "\n",
    "Um dies zu verdeutlichen, vergleichen wir die Dauer der Berechnungen bei $T_1(n) = 10000 \\cdot n^2$ und $T_2(n) = 2^n$, unter der Annahme, dass der Computer $10^9$ Operationen in der Sekunde durchführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        T1(n)                  T2(n)\n",
      "5    0.00025s                   0.0s\n",
      "10     0.001s                   0.0s\n",
      "15   0.00225s                 3e-05s\n",
      "20     0.004s               0.00105s\n",
      "25   0.00625s               0.03355s\n",
      "30     0.009s               1.07374s\n",
      "35   0.01225s              34.35974s\n",
      "40     0.016s            18.32519min\n",
      "45   0.02025s               9.77344h\n",
      "50     0.025s                    13d\n",
      "55   0.03025s                   1yrs\n",
      "60     0.036s                  36yrs\n",
      "65   0.04225s               1,169yrs\n",
      "70     0.049s              37,436yrs\n",
      "75   0.05625s           1,197,962yrs\n",
      "80     0.064s          38,334,786yrs\n",
      "85   0.07225s       1,226,713,160yrs\n",
      "90     0.081s      39,254,821,134yrs\n",
      "95   0.09025s   1,256,154,276,291yrs\n",
      "100      0.1s  40,196,936,841,331yrs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def T1(n):\n",
    "    return 10000 * (n**2)\n",
    "\n",
    "\n",
    "def T2(n):\n",
    "    return 2**n\n",
    "\n",
    "\n",
    "def time(func, n, ops_per_sec):\n",
    "    ops = func(n)\n",
    "    if ops > sys.float_info.max:\n",
    "        t = ops // ops_per_sec\n",
    "    else:\n",
    "        t = ops / ops_per_sec\n",
    "    if t >= 365 * 24 * 60 * 60:\n",
    "        return '{:,}yrs'.format(round(t // (365 * 24 * 60 * 60)))\n",
    "    if t >= 24 * 60 * 60:\n",
    "        return str(round(t // (24 * 60 * 60))) + 'd'\n",
    "    if t >= 60 * 60:\n",
    "        return str(round((t / (60 * 60)), 5)) + 'h'\n",
    "    if t >= 60:\n",
    "        return str(round((t/60), 5)) + 'min'\n",
    "    return str(round(t, 5)) + 's'\n",
    "\n",
    "\n",
    "n = list(range(5, 101, 5))\n",
    "ops_per_sec = 10**9\n",
    "T1_list = []\n",
    "T2_list = []\n",
    "\n",
    "for i in n:\n",
    "    T1_list.append(time(T1, i, ops_per_sec))\n",
    "    T2_list.append(time(T2, i, ops_per_sec))\n",
    "\n",
    "\n",
    "print(pd.DataFrame({'T1(n)': pd.Series(T1_list, index=n), 'T2(n)': pd.Series(T2_list, index=n)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während $T2$ für sehr kleine Werte günstigere Laufzeiten liefert als $T1$ und zunächst nicht auf einen großen Anstieg der Laufzeit schließen lässt, wird doch relativ schnell klar, wie langsam der Algorithmus bei $T2$ ist. Sobald $n$ größer wird, kommt man sehr schnell in einen Bereich, bei dem es keinen Sinn mehr macht, auf das Ergebnis zu warten. Bereits bei $n=100$ werden über 40 Billionen Jahre benötigt.\n",
    "\n",
    "Ferner kann man zeigen, dass jede Exponentialfunktion $f(n) = a^n$ mit $a > 1$ für $n \\to \\infty$ schneller wächst als jede Polynomialfunktion $g(n) = b \\cdot n^c$ mit $c \\in \\mathbb{N}$ für $n \\to \\infty$:\n",
    "\n",
    "$$\\text{Behauptung:} \\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\infty$$\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\mathrm{e}^{\\ln(a) \\cdot n}}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot n^c}}{\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}}}$$\n",
    "\n",
    "Da hier sowohl Zähler als auch Nenner gegen 0 streben, kann die Regel von l'Hospital angewandt werden:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{d}{d n}(\\frac{1}{b \\cdot n^c})}{\\frac{d}{d n}(\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}})} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{\\frac{d}{d n}(b \\cdot n^c)}}{\\frac{1}{\\frac{d}{d n}(\\mathrm{e}^{\\ln(a) \\cdot n})}} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c \\cdot n^{c-1}}}{\\frac{1}{\\ln(a) \\cdot a^n}} $$\n",
    "\n",
    "Da weiterhin sowohl Zähler als auch Nenner gegen 0 streben, muss die Regel von l'Hospital solange angewandt werden, bis dies bei einem der beiden Terme nicht mehr der Fall ist:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c!}}{\\frac{1}{\\ln^c(a) \\cdot a^n}} = \\lim_{n \\to \\infty} \\frac{\\ln^c(a) \\cdot a^n}{b \\cdot c!} = \\infty$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
